name: Test

on:
  push:
    branches: [main]
  pull_request:
    branches: [main, release]
  workflow_dispatch:

concurrency:
  group: test-${{ github.head_ref || github.ref }}
  cancel-in-progress: true

jobs:
  # Detect which paths changed to skip irrelevant jobs
  changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
    steps:
      - uses: actions/checkout@v6
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            backend:
              - 'backend/**'
              - 'satellite_processor/**'
              - 'requirements*.txt'
              - 'backend/requirements.txt'
            frontend:
              - 'frontend/**'

  lint-audit:
    name: Lint & Security Audit
    runs-on: ubuntu-latest
    needs: [changes]
    if: needs.changes.outputs.backend == 'true' || github.event_name == 'push'
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        run: |
          pip install uv
          uv pip install --system -r backend/requirements.txt
          uv pip install --system ruff pip-audit

      - name: Lint with ruff
        run: ruff check backend/

      - name: Security audit (pip-audit)
        run: pip-audit --strict --ignore-vuln PYSEC-2024-0 || true

  backend:
    name: Backend Tests (Shard ${{ matrix.shard }}/4)
    runs-on: ubuntu-latest
    needs: [changes]
    if: needs.changes.outputs.backend == 'true' || github.event_name == 'push'
    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2, 3, 4]
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: backend/requirements.txt

      - name: Install system dependencies
        run: |
          # These packages are pre-installed on ubuntu-24.04 runners
          # Only install if missing (avoids slow apt-get update)
          for pkg in ffmpeg libgl1 libglib2.0-0; do
            dpkg -s "$pkg" >/dev/null 2>&1 || NEED_INSTALL=1
          done
          if [ "${NEED_INSTALL:-}" = "1" ]; then
            sudo apt-get update -qq && sudo apt-get install -y -qq ffmpeg libgl1 libglib2.0-0
          fi

      - name: Install Python dependencies
        run: |
          pip install uv
          uv pip install --system -r backend/requirements.txt

      - name: Run backend tests with coverage
        working-directory: backend
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: |
          pytest --splits 4 --group ${{ matrix.shard }} \
            --splitting-algorithm least_duration \
            --cov=app --cov-report=xml:coverage-shard${{ matrix.shard }}.xml \
            --cov-fail-under=60 -n auto --durations=10 --reruns 1 \
            -v --tb=short -q \
            --store-durations \
            tests/

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: backend-coverage-shard${{ matrix.shard }}
          path: backend/coverage-shard${{ matrix.shard }}.xml
          retention-days: 1

  integration:
    name: Integration & Migration Tests
    runs-on: ubuntu-latest
    needs: [changes]
    if: needs.changes.outputs.backend == 'true' || github.event_name == 'push'
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: backend/requirements.txt

      - name: Start PostgreSQL
        run: |
          sudo systemctl start postgresql
          sudo -u postgres createuser -s test
          sudo -u postgres psql -c "ALTER USER test PASSWORD 'test';"
          sudo -u postgres createdb -O test test_sat

      - name: Start Redis
        run: |
          sudo apt-get install -y -qq redis-server
          redis-server --daemonize yes

      - name: Install system dependencies
        run: |
          for pkg in ffmpeg libgl1 libglib2.0-0; do
            dpkg -s "$pkg" >/dev/null 2>&1 || NEED_INSTALL=1
          done
          if [ "${NEED_INSTALL:-}" = "1" ]; then
            sudo apt-get update -qq && sudo apt-get install -y -qq ffmpeg libgl1 libglib2.0-0
          fi

      - name: Install Python dependencies
        run: |
          pip install uv
          uv pip install --system -r backend/requirements.txt

      - name: Run alembic migrations
        working-directory: backend
        env:
          DATABASE_URL: postgresql+psycopg2://test:test@localhost:5432/test_sat
        run: alembic upgrade head

      - name: Run integration tests
        working-directory: backend
        env:
          DATABASE_URL: postgresql+asyncpg://test:test@localhost:5432/test_sat
          REDIS_URL: redis://localhost:6379/0
          PYTHONPATH: ${{ github.workspace }}
        run: pytest -v --tb=short -m integration tests/test_integration.py --durations=10 --reruns 1

      - name: Run core processor tests
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: pytest satellite_processor/core/tests/ -v --tb=short --cov --cov-config=pyproject.toml --cov-report=xml:core-coverage.xml --durations=10 --reruns 1 --cov-fail-under=60

      - name: Upload core coverage artifact
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: core-coverage
          path: core-coverage.xml
          retention-days: 1

  api-contracts:
    name: API Contract Validation
    runs-on: [self-hosted, ci]
    needs: [changes]
    if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.frontend == 'true' || github.event_name == 'push'
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        run: |
          pip install uv
          uv pip install --system -r backend/requirements.txt

      - name: Validate API contracts
        working-directory: backend
        run: python3 ../scripts/validate_api_contracts.py

  integration-smoke:
    name: Docker Compose Smoke Test
    runs-on: ubuntu-latest
    needs: [changes]
    if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.frontend == 'true' || github.event_name == 'push'
    env:
      POSTGRES_PASSWORD: testpassword
      POSTGRES_USER: sat
      POSTGRES_DB: satellite_processor
      API_KEY: test-api-key
      CORS_ORIGINS: '["http://localhost:3000"]'
    steps:
      - uses: actions/checkout@v6

      - name: Build images
        run: docker compose build

      - name: Start services
        run: docker compose up -d

      - name: Wait for API health
        run: |
          echo "Waiting for API to be healthy..."
          for i in $(seq 1 60); do
            if curl -sf http://localhost:8000/api/health > /dev/null 2>&1; then
              echo "API healthy after ${i}s"
              exit 0
            fi
            sleep 1
          done
          echo "API failed to become healthy within 60s"
          docker compose logs api
          exit 1

      - name: Run smoke tests
        run: bash scripts/smoke_test.sh http://localhost:8000 test-api-key

      - name: Cleanup
        if: always()
        run: docker compose down -v

  frontend:
    name: Frontend Tests (Shard ${{ matrix.shard }}/2)
    runs-on: ubuntu-latest
    needs: [changes]
    if: needs.changes.outputs.frontend == 'true' || github.event_name == 'push'
    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2]
    steps:
      - uses: actions/checkout@v6

      - name: Set up Node
        uses: actions/setup-node@v6
        with:
          node-version: "20"
          cache: npm
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        working-directory: frontend
        run: npm ci

      - name: Lint
        if: matrix.shard == 1
        working-directory: frontend
        run: npm run lint

      - name: Unit Tests with coverage
        working-directory: frontend
        run: npx vitest run --coverage --shard=${{ matrix.shard }}/2

      - name: Upload coverage artifact
        uses: actions/upload-artifact@v5
        with:
          name: frontend-coverage-shard${{ matrix.shard }}
          path: frontend/coverage/lcov.info
          retention-days: 1

  frontend-build:
    name: Frontend Build
    runs-on: ubuntu-latest
    needs: [changes]
    if: needs.changes.outputs.frontend == 'true' || github.event_name == 'push'
    steps:
      - uses: actions/checkout@v6

      - name: Set up Node
        uses: actions/setup-node@v6
        with:
          node-version: "20"
          cache: npm
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        working-directory: frontend
        run: npm ci

      - name: Build
        working-directory: frontend
        run: npm run build

      - name: Upload build artifact
        uses: actions/upload-artifact@v5
        with:
          name: frontend-build
          path: frontend/dist/
          retention-days: 1

  e2e:
    name: E2E Tests (Shard ${{ matrix.shard }}/3)
    runs-on: ubuntu-latest
    needs: [frontend-build]
    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2, 3]
        total: [3]
    steps:
      - uses: actions/checkout@v6

      - name: Set up Node
        uses: actions/setup-node@v6
        with:
          node-version: "20"
          cache: npm
          cache-dependency-path: frontend/package-lock.json

      - name: Cache node modules
        uses: actions/cache@v5
        with:
          path: frontend/node_modules
          key: node-modules-${{ hashFiles('frontend/package-lock.json') }}

      - name: Install dependencies
        working-directory: frontend
        run: npm ci

      - name: Download build artifact
        uses: actions/download-artifact@v7
        with:
          name: frontend-build
          path: frontend/dist/

      - name: Cache Playwright browsers
        id: playwright-cache
        uses: actions/cache@v5
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ hashFiles('frontend/package-lock.json') }}

      - name: Install Playwright Browsers
        working-directory: frontend
        run: npx playwright install --with-deps chromium

      - name: Run E2E tests
        working-directory: frontend
        run: npx playwright test --shard=${{ matrix.shard }}/${{ matrix.total }}
        env:
          CI: true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: playwright-report-shard${{ matrix.shard }}
          path: frontend/playwright-report/
          retention-days: 7

      - name: Upload Playwright traces on failure
        if: failure()
        uses: actions/upload-artifact@v5
        with:
          name: playwright-traces-shard${{ matrix.shard }}
          path: frontend/test-results/
          retention-days: 7

  # SonarQube analysis ‚Äî runs after tests, even if they fail
  sonarqube:
    name: SonarQube Scan
    needs: [changes, backend, frontend]
    if: always() && needs.changes.result == 'success'
    runs-on: [self-hosted, sonar]
    timeout-minutes: 10
    concurrency:
      group: sonar-${{ github.head_ref || github.ref_name }}
      cancel-in-progress: true
    permissions:
      contents: read
      pull-requests: write
      statuses: write
    steps:
      - name: Clean workspace and scanner cache
        run: |
          rm -rf "$GITHUB_WORKSPACE"/* "$GITHUB_WORKSPACE"/.* 2>/dev/null || true
          rm -rf /tmp/runner/work/satellite_processor/satellite_processor/.scannerwork 2>/dev/null || true
          rm -rf "$HOME/.sonar" 2>/dev/null || true

      - uses: actions/checkout@v6
        with:
          fetch-depth: 0
          clean: true

      - name: Verify no contamination
        run: |
          echo "Checking for contaminated directories..."
          for dir in src receiver; do
            if [ -d "$dir" ]; then
              echo "ERROR: Found contaminated directory: $dir"
              rm -rf "$dir"
            fi
          done
          echo "Source files found:"
          find . -name "*.ts" -o -name "*.tsx" -o -name "*.py" | grep -v node_modules | grep -v .venv | wc -l

      - name: Download backend coverage shards
        id: dl-backend
        uses: actions/download-artifact@v7
        continue-on-error: true
        with:
          pattern: backend-coverage-shard*
          path: backend/

      - name: Flatten backend coverage
        run: |
          find backend/ -mindepth 2 -name "*.xml" -exec mv {} backend/ \; 2>/dev/null || true

      - name: Download frontend coverage shards
        id: dl-frontend
        uses: actions/download-artifact@v7
        continue-on-error: true
        with:
          pattern: frontend-coverage-shard*
          path: frontend/coverage-shards/

      - name: Merge frontend coverage
        run: |
          mkdir -p frontend/coverage
          cat frontend/coverage-shards/*/lcov.info > frontend/coverage/lcov.info 2>/dev/null || true
          if [ ! -f frontend/coverage/lcov.info ]; then
            echo "TN:" > frontend/coverage/lcov.info
          fi

      - name: Download core coverage
        id: dl-core
        uses: actions/download-artifact@v7
        continue-on-error: true
        with:
          name: core-coverage
          path: .

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.12'

      - name: Merge backend coverage
        working-directory: backend
        run: |
          if ls coverage-shard*.xml 1>/dev/null 2>&1; then
            python3 ../scripts/merge_coverage.py
          elif [ ! -f coverage.xml ]; then
            echo '<?xml version="1.0" ?><coverage version="6" lines-valid="0" lines-covered="0" line-rate="0" branches-covered="0" branches-valid="0" branch-rate="0" timestamp="0" complexity="0"><sources><source>backend/app</source></sources><packages></packages></coverage>' > coverage.xml
          fi

      - name: Ensure coverage files exist (fallback placeholders)
        run: |
          if [ ! -f backend/coverage.xml ]; then
            echo '<?xml version="1.0" ?><coverage version="6" lines-valid="0" lines-covered="0" line-rate="0" branches-covered="0" branches-valid="0" branch-rate="0" timestamp="0" complexity="0"><sources><source>backend/app</source></sources><packages></packages></coverage>' > backend/coverage.xml
          fi
          if [ ! -f frontend/coverage/lcov.info ]; then
            mkdir -p frontend/coverage
            echo "TN:" > frontend/coverage/lcov.info
          fi
          if [ ! -f core-coverage.xml ]; then
            echo '<?xml version="1.0" ?><coverage version="6" lines-valid="0" lines-covered="0" line-rate="0" branches-covered="0" branches-valid="0" branch-rate="0" timestamp="0" complexity="0"><sources><source>satellite_processor/core</source></sources><packages></packages></coverage>' > core-coverage.xml
          fi

      - name: Resolve PR number
        id: pr-info
        if: github.event_name == 'workflow_dispatch'
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          PR=$(gh pr list --head "${{ github.ref_name }}" --json number -q '.[0].number // empty')
          if [ -n "$PR" ]; then
            echo "number=$PR" >> "$GITHUB_OUTPUT"
            echo "branch=${{ github.ref_name }}" >> "$GITHUB_OUTPUT"
            echo "base=main" >> "$GITHUB_OUTPUT"
          fi

      - name: Build sonar args
        id: sonar-args
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "args=-Dsonar.pullrequest.key=${{ github.event.pull_request.number }} -Dsonar.pullrequest.branch=${{ github.head_ref }} -Dsonar.pullrequest.base=${{ github.base_ref }}" >> "$GITHUB_OUTPUT"
          elif [ -n "${{ steps.pr-info.outputs.number }}" ]; then
            echo "args=-Dsonar.pullrequest.key=${{ steps.pr-info.outputs.number }} -Dsonar.pullrequest.branch=${{ steps.pr-info.outputs.branch }} -Dsonar.pullrequest.base=${{ steps.pr-info.outputs.base }}" >> "$GITHUB_OUTPUT"
          else
            echo "args=" >> "$GITHUB_OUTPUT"
          fi

      - name: SonarQube Scan
        uses: sonarsource/sonarqube-scan-action@v7
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        with:
          args: ${{ steps.sonar-args.outputs.args }}

      - name: SonarQube Quality Gate
        id: quality-gate
        uses: sonarsource/sonarqube-quality-gate-action@v1
        timeout-minutes: 5
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}

      - name: Post SonarQube summary to PR
        if: always() && (github.event_name == 'pull_request' || steps.pr-info.outputs.number)
        uses: actions/github-script@v8
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        with:
          script: |
            const prNumber = context.payload.pull_request?.number || parseInt('${{ steps.pr-info.outputs.number }}', 10);
            const sonarUrl = process.env.SONAR_HOST_URL;
            const sonarToken = process.env.SONAR_TOKEN;
            const projectKey = 'satellite_processor';
            const commitSha = context.sha;

            const auth = Buffer.from(sonarToken + ':').toString('base64');
            const headers = { 'Authorization': `Basic ${auth}` };

            // --- Helpers ---
            const ratingMap = { '1.0': 'A', '2.0': 'B', '3.0': 'C', '4.0': 'D', '5.0': 'E' };
            const rating = (val) => ratingMap[val] || val || 'N/A';
            const num = (val) => val ?? 'N/A';
            const pct = (val) => val ? `${parseFloat(val).toFixed(1)}%` : 'N/A';

            function formatDebt(minutes) {
              if (!minutes || minutes === 'N/A') return 'N/A';
              const m = parseInt(minutes);
              if (isNaN(m)) return 'N/A';
              if (m < 60) return `${m}m`;
              const h = Math.floor(m / 60);
              const rem = m % 60;
              if (h < 24) return rem > 0 ? `${h}h ${rem}m` : `${h}h`;
              const d = Math.floor(h / 24);
              const remH = h % 24;
              return remH > 0 ? `${d}d ${remH}h` : `${d}d`;
            }

            // --- Fetch PR quality gate & measures ---
            let measures = {};
            let gateStatus = 'UNKNOWN';

            try {
              const gateRes = await fetch(`${sonarUrl}/api/qualitygates/project_status?projectKey=${projectKey}&pullRequest=${prNumber}`, { headers });
              if (gateRes.ok) {
                const gateData = await gateRes.json();
                gateStatus = gateData.projectStatus?.status || 'UNKNOWN';
              }

              const metricsToFetch = [
                'new_bugs', 'new_vulnerabilities', 'new_code_smells',
                'new_coverage', 'new_duplicated_lines_density',
                'new_security_hotspots', 'new_reliability_rating',
                'new_security_rating', 'new_maintainability_rating',
                'new_technical_debt'
              ].join(',');

              const measuresRes = await fetch(`${sonarUrl}/api/measures/component?component=${projectKey}&pullRequest=${prNumber}&metricKeys=${metricsToFetch}`, { headers });
              if (measuresRes.ok) {
                const measuresData = await measuresRes.json();
                for (const m of (measuresData.component?.measures || [])) {
                  measures[m.metric] = m.period?.value ?? m.value ?? 'N/A';
                }
              }
            } catch (e) {
              console.log('Failed to fetch SonarQube data:', e.message);
            }

            // --- Fetch OVERALL project measures ---
            let overallMeasures = {};
            try {
              const overallMetrics = ['coverage', 'ncloc', 'bugs', 'vulnerabilities', 'code_smells', 'duplicated_lines_density', 'sqale_index'].join(',');
              const overallRes = await fetch(`${sonarUrl}/api/measures/component?component=${projectKey}&metricKeys=${overallMetrics}`, { headers });
              if (overallRes.ok) {
                const overallData = await overallRes.json();
                for (const m of (overallData.component?.measures || [])) {
                  overallMeasures[m.metric] = m.value ?? 'N/A';
                }
              }
            } catch (e) {
              console.log('Failed to fetch overall metrics:', e.message);
            }

            // --- Coverage trend ---
            let coverageTrend = '';
            try {
              const historyRes = await fetch(`${sonarUrl}/api/measures/search_history?component=${projectKey}&metrics=coverage&ps=5`, { headers });
              if (historyRes.ok) {
                const histData = await historyRes.json();
                const values = histData.measures?.[0]?.history || [];
                if (values.length >= 2) {
                  const current = parseFloat(values[values.length - 1].value);
                  const previous = parseFloat(values[values.length - 2].value);
                  const delta = current - previous;
                  const arrow = delta > 0 ? '‚Üë' : delta < 0 ? '‚Üì' : '‚Üí';
                  const sign = delta > 0 ? '+' : '';
                  coverageTrend = ` (${arrow} ${sign}${delta.toFixed(1)}% from previous)`;
                }
              }
            } catch (e) {
              console.log('Coverage history failed:', e.message);
            }

            // --- Fetch PR issues ---
            let issues = [];
            try {
              const issuesRes = await fetch(
                `${sonarUrl}/api/issues/search?componentKeys=${projectKey}&pullRequest=${prNumber}&statuses=OPEN,CONFIRMED&ps=50&s=SEVERITY&asc=false`,
                { headers }
              );
              if (issuesRes.ok) {
                const issuesData = await issuesRes.json();
                issues = issuesData.issues || [];
              }
            } catch (e) {
              console.log('Failed to fetch issues:', e.message);
            }

            // --- Overall issues breakdown ---
            let overallIssues = [];
            try {
              const oiRes = await fetch(
                `${sonarUrl}/api/issues/search?componentKeys=${projectKey}&statuses=OPEN&ps=50&s=SEVERITY&asc=false`,
                { headers }
              );
              if (oiRes.ok) {
                const oiData = await oiRes.json();
                overallIssues = oiData.issues || [];
              }
            } catch (e) {
              console.log('Failed to fetch overall issues:', e.message);
            }

            // --- Get changed files ---
            let changedFiles = [];
            try {
              const filesRes = await github.rest.pulls.listFiles({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: prNumber,
                per_page: 100,
              });
              changedFiles = filesRes.data.map(f => f.filename);
            } catch (e) {
              console.log('Failed to fetch PR files:', e.message);
            }

            // --- Fetch per-file coverage ---
            let fileMetrics = [];
            if (changedFiles.length > 0) {
              try {
                const componentTree = await fetch(
                  `${sonarUrl}/api/measures/component_tree?component=${projectKey}&pullRequest=${prNumber}&metricKeys=coverage,ncloc&ps=100&strategy=leaves`,
                  { headers }
                );
                if (componentTree.ok) {
                  const treeData = await componentTree.json();
                  const components = treeData.components || [];
                  for (const comp of components) {
                    const sonarPath = comp.path || comp.key.replace(`${projectKey}:`, '');
                    if (/\/(tests?|__tests__|spec|__mocks__)\//i.test(sonarPath) || /\.(config|spec|test)\.[jt]sx?$/.test(sonarPath)) continue;
                    if (changedFiles.some(f => sonarPath.endsWith(f) || f.endsWith(sonarPath))) {
                      const cov = comp.measures?.find(m => m.metric === 'coverage');
                      const lines = comp.measures?.find(m => m.metric === 'ncloc');
                      if (cov || lines) {
                        fileMetrics.push({
                          file: sonarPath,
                          coverage: cov?.value ?? 'N/A',
                          lines: lines?.value ?? 'N/A',
                        });
                      }
                    }
                  }
                }
              } catch (e) {
                console.log('Failed to fetch file metrics:', e.message);
              }
            }

            // --- Build per-file coverage section ---
            let filesSection = '';
            if (fileMetrics.length > 0) {
              fileMetrics.sort((a, b) => (parseFloat(a.coverage) || 0) - (parseFloat(b.coverage) || 0));
              const fileRows = fileMetrics.map(f => {
                const covVal = f.coverage !== 'N/A' ? parseFloat(f.coverage).toFixed(1) + '%' : 'N/A';
                const icon = f.coverage !== 'N/A' ? (parseFloat(f.coverage) >= 80 ? '‚úÖ' : parseFloat(f.coverage) >= 50 ? '‚ö†Ô∏è' : '‚ùå') : '‚ùì';
                const shortPath = f.file.length > 60 ? '...' + f.file.slice(-57) : f.file;
                return `| ${icon} | \`${shortPath}\` | ${covVal} | ${f.lines} |`;
              }).join('\n');

              filesSection = [
                '', '### Files Changed',
                '| | File | Coverage | Lines |',
                '|---|------|----------|-------|',
                fileRows,
              ].join('\n');
            }

            const gateIcon = gateStatus === 'OK' ? '‚úÖ' : gateStatus === 'ERROR' ? '‚ùå' : '‚ö†Ô∏è';

            // --- Badge URLs ---
            const covVal = overallMeasures.coverage ? parseFloat(overallMeasures.coverage).toFixed(1) : '0';
            const covColor = parseFloat(covVal) >= 80 ? 'green' : parseFloat(covVal) >= 50 ? 'yellow' : 'red';
            const bugsVal = overallMeasures.bugs || '0';
            const bugsColor = parseInt(bugsVal) === 0 ? 'green' : parseInt(bugsVal) > 5 ? 'red' : 'orange';
            const smellsVal = overallMeasures.code_smells || '0';
            const gateColor = gateStatus === 'OK' ? 'green' : 'red';
            const gateLabel = gateStatus === 'OK' ? 'passed' : gateStatus === 'ERROR' ? 'failed' : gateStatus.toLowerCase();

            const badges = [
              `![Coverage](https://img.shields.io/badge/coverage-${encodeURIComponent(covVal + '%')}-${covColor})`,
              `![Bugs](https://img.shields.io/badge/bugs-${encodeURIComponent(bugsVal)}-${bugsColor})`,
              `![Smells](https://img.shields.io/badge/smells-${encodeURIComponent(smellsVal)}-yellow)`,
              `![Gate](https://img.shields.io/badge/gate-${encodeURIComponent(gateLabel)}-${gateColor})`,
            ].join(' ');

            // --- PR Issues section ---
            let issuesSection = '';
            if (issues.length > 0) {
              const typeIcon = { BUG: 'üêõ', VULNERABILITY: 'üîì', CODE_SMELL: 'üßπ', SECURITY_HOTSPOT: 'üî•' };
              const bugCount = issues.filter(i => i.type === 'BUG').length;
              const vulnCount = issues.filter(i => i.type === 'VULNERABILITY').length;
              const smellCount = issues.filter(i => i.type === 'CODE_SMELL').length;
              const hotspotCount = issues.filter(i => i.type === 'SECURITY_HOTSPOT').length;

              const summaryParts = [];
              if (bugCount) summaryParts.push(`üêõ ${bugCount} Bug${bugCount > 1 ? 's' : ''}`);
              if (vulnCount) summaryParts.push(`üîì ${vulnCount} Vulnerabilit${vulnCount > 1 ? 'ies' : 'y'}`);
              if (hotspotCount) summaryParts.push(`üî• ${hotspotCount} Hotspot${hotspotCount > 1 ? 's' : ''}`);
              if (smellCount) summaryParts.push(`üßπ ${smellCount} Code Smell${smellCount > 1 ? 's' : ''}`);

              const totalIssues = issues.length;
              const shownIssues = issues.slice(0, 20);
              const rows = shownIssues.map(i => {
                const component = (i.component || '').replace(`${projectKey}:`, '');
                const loc = i.line ? `${component}:${i.line}` : component;
                return `| ${typeIcon[i.type] || '‚ùì'} | ${i.severity || 'N/A'} | \`${loc}\` | ${(i.message || '').substring(0, 100)} |`;
              }).join('\n');

              const moreText = totalIssues > 20 ? `\n\n*... and ${totalIssues - 20} more ‚Äî [view all on SonarQube](${sonarUrl}/project/issues?id=${projectKey}&pullRequest=${prNumber})*` : '';

              issuesSection = [
                '', `### Issues (${totalIssues} found)`,
                '<details>',
                `<summary>${summaryParts.join(' ¬∑ ')}</summary>`,
                '',
                '| Type | Severity | File | Description |',
                '|------|----------|------|-------------|',
                rows, moreText, '', '</details>'
              ].join('\n');
            }

            // --- Overall issues breakdown section ---
            let overallIssuesSection = '';
            if (overallIssues.length > 0) {
              const bugs = overallIssues.filter(i => i.type === 'BUG');
              const smells = overallIssues.filter(i => i.type === 'CODE_SMELL');
              const vulns = overallIssues.filter(i => i.type === 'VULNERABILITY');

              const totalOpen = overallIssues.length;
              const summaryLine = `> üêõ ${bugs.length} Bug${bugs.length !== 1 ? 's' : ''} ¬∑ üßπ ${smells.length} Code Smell${smells.length !== 1 ? 's' : ''} ¬∑ üîì ${vulns.length} Vulnerabilit${vulns.length !== 1 ? 'ies' : 'y'}`;

              function issueTable(items, label, icon, cap = 15) {
                if (items.length === 0) return '';
                const shown = items.slice(0, cap);
                const showingNote = items.length > cap ? ` (showing top ${cap})` : '';
                const rows = shown.map(i => {
                  const component = (i.component || '').replace(`${projectKey}:`, '');
                  const loc = i.line ? `${component}:${i.line}` : component;
                  return `| ${i.severity || 'N/A'} | \`${loc}\` | ${(i.message || '').substring(0, 80)} |`;
                }).join('\n');
                const viewAll = items.length > cap ? `\n\n[View all on SonarQube](${sonarUrl}/project/issues?id=${projectKey}&types=${items[0].type}&statuses=OPEN)` : '';
                return [
                  '<details>',
                  `<summary>${icon} ${items.length} ${label}${showingNote}</summary>`,
                  '',
                  '| Severity | File | Description |',
                  '|----------|------|-------------|',
                  rows,
                  viewAll,
                  '', '</details>'
                ].join('\n');
              }

              overallIssuesSection = [
                '', `### Overall Issues (${totalOpen} open)`,
                summaryLine, '',
                issueTable(bugs, 'Bugs', 'üêõ'),
                issueTable(smells, 'Code Smells', 'üßπ'),
                issueTable(vulns, 'Vulnerabilities', 'üîì'),
              ].filter(Boolean).join('\n');
            }

            // --- Build final comment body with dedup marker ---
            const body = [
              '<!-- sonarqube-summary -->',
              `## ${gateIcon} SonarQube Analysis`,
              '',
              badges,
              '',
              `**Quality Gate:** ${gateStatus === 'OK' ? 'Passed' : gateStatus === 'ERROR' ? 'Failed' : gateStatus}`,
              '',
              '### New Code',
              '| Metric | Value |',
              '|--------|-------|',
              `| üêõ Bugs | ${num(measures.new_bugs)} |`,
              `| üîì Vulnerabilities | ${num(measures.new_vulnerabilities)} |`,
              `| üî• Security Hotspots | ${num(measures.new_security_hotspots)} |`,
              `| üßπ Code Smells | ${num(measures.new_code_smells)} |`,
              `| üìä Coverage | ${pct(measures.new_coverage)} |`,
              `| üìã Duplication | ${pct(measures.new_duplicated_lines_density)} |`,
              `| üïê New Debt | ${measures.new_technical_debt && measures.new_technical_debt !== 'N/A' ? '+' + formatDebt(measures.new_technical_debt) : 'N/A'} |`,
              '',
              '### Ratings',
              '| | Reliability | Security | Maintainability |',
              '|---|---|---|---|',
              `| Grade | ${rating(measures.new_reliability_rating)} | ${rating(measures.new_security_rating)} | ${rating(measures.new_maintainability_rating)} |`,
              filesSection,
              issuesSection,
              '',
              '### Overall Project',
              '| Metric | Value |',
              '|--------|-------|',
              `| üìä Coverage | ${pct(overallMeasures.coverage)}${coverageTrend} |`,
              `| üïê Technical Debt | ${formatDebt(overallMeasures.sqale_index)} |`,
              `| üìè Lines of Code | ${num(overallMeasures.ncloc)} |`,
              `| üêõ Bugs | ${num(overallMeasures.bugs)} |`,
              `| üîì Vulnerabilities | ${num(overallMeasures.vulnerabilities)} |`,
              `| üßπ Code Smells | ${num(overallMeasures.code_smells)} |`,
              `| üìã Duplication | ${pct(overallMeasures.duplicated_lines_density)} |`,
              overallIssuesSection,
              '',
              `[View full analysis on SonarQube](${sonarUrl}/dashboard?id=${projectKey}&pullRequest=${prNumber})`,
              '',
              '---',
              `*SonarQube ¬∑ \`${commitSha.substring(0, 7)}\` ¬∑ ${new Date().toISOString().replace('T', ' ').substring(0, 19)} UTC*`
            ].join('\n');

            // --- Dedup: find existing comment with marker, update or create ---
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
            });

            const existing = comments.find(c =>
              c.user.login === 'github-actions[bot]' && c.body.includes('<!-- sonarqube-summary -->')
            );

            // Delete old SonarQube comment if it exists, then post fresh
            if (existing) {
              await github.rest.issues.deleteComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
              });
            }

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body,
            });

            // --- Inline annotations: delete old ones, post fresh ---
            try {
              // Delete all existing review comments by github-actions[bot]
              const { data: reviewComments } = await github.rest.pulls.listReviewComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: prNumber,
                per_page: 100,
              });

              const botReviewComments = reviewComments.filter(c =>
                c.user.login === 'github-actions[bot]' && c.body.includes('SonarQube')
              );

              for (const rc of botReviewComments) {
                await github.rest.pulls.deleteReviewComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: rc.id,
                });
              }

              // Post fresh annotations
              const prIssues = issues.filter(i => i.line && i.component);
              let annotated = 0;
              for (const issue of prIssues) {
                try {
                  const typeIcon = { BUG: 'üêõ', VULNERABILITY: 'üîì', CODE_SMELL: 'üßπ', SECURITY_HOTSPOT: 'üî•' };
                  const icon = typeIcon[issue.type] || '‚ùì';
                  await github.rest.pulls.createReviewComment({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    pull_number: prNumber,
                    body: `${icon} **SonarQube ${issue.severity}**: ${issue.message}\n\n[View in SonarQube](${sonarUrl}/project/issues?id=${projectKey}&open=${issue.key})`,
                    commit_id: commitSha,
                    path: issue.component.replace(`${projectKey}:`, ''),
                    line: issue.line,
                  });
                  annotated++;
                } catch (e) {
                  console.log(`Skipped annotation for ${issue.key}: ${e.message}`);
                }
              }
              if (annotated > 0) {
                console.log(`Posted ${annotated} inline review annotations`);
              }
            } catch (e) {
              console.log('Inline annotations failed:', e.message);
            }

      - name: Post commit status
        if: always()
        uses: actions/github-script@v8
        with:
          script: |
            const sha = context.sha;
            const gate = '${{ steps.quality-gate.outcome }}';
            let state, description;
            if (gate === 'success') {
              state = 'success';
              description = 'Quality gate passed';
            } else if (gate === 'failure') {
              state = 'failure';
              description = 'Quality gate failed';
            } else {
              state = 'error';
              description = 'Quality gate check did not complete';
            }
            const runUrl = `${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID}`;

            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: sha,
              state: state,
              target_url: runUrl,
              description: description,
              context: 'SonarQube Quality Gate'
            });

  # Consolidate coverage from all shards + frontend and post PR summary
  pr-summary:
    name: PR Summary
    runs-on: ubuntu-latest
    needs: [changes, backend, frontend, frontend-build, e2e, integration, lint-audit]
    if: always() && github.event_name == 'pull_request'
    permissions:
      pull-requests: write
    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Download backend coverage shards
        if: needs.backend.result == 'success'
        uses: actions/download-artifact@v7
        continue-on-error: true
        with:
          pattern: backend-coverage-*
          path: backend/
          merge-multiple: true

      - name: Download frontend coverage shards
        if: needs.frontend.result == 'success'
        uses: actions/download-artifact@v7
        continue-on-error: true
        with:
          pattern: frontend-coverage-shard*
          path: frontend/coverage-shards/
          merge-multiple: false

      - name: Merge frontend coverage
        if: needs.frontend.result == 'success'
        run: |
          mkdir -p frontend/coverage
          cat frontend/coverage-shards/frontend-coverage-shard*/lcov.info > frontend/coverage/lcov.info 2>/dev/null || true

      - name: Merge backend coverage
        run: |
          if [ -f backend/coverage-shard1.xml ]; then
            cd backend
            python3 ../scripts/merge_coverage.py
            python3 -c "
          import xml.etree.ElementTree as ET
          root = ET.parse('coverage.xml').getroot()
          rate = root.get('line-rate', '0')
          print(round(float(rate) * 100, 1))
          " > coverage-total.txt 2>/dev/null || echo "0" > coverage-total.txt
            cat coverage-total.txt
          else
            echo "N/A" > backend/coverage-total.txt
          fi

      - name: Upload merged coverage
        if: needs.backend.result == 'success'
        uses: actions/upload-artifact@v5
        with:
          name: backend-coverage-merged
          path: backend/coverage.xml
          retention-days: 1

      - name: Post PR comment
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');

            // Read backend coverage
            let backendCov = 'N/A';
            try {
              backendCov = fs.readFileSync('backend/coverage-total.txt', 'utf8').trim();
              if (backendCov !== 'N/A') backendCov += '%';
            } catch {}

            // Read frontend coverage from lcov
            let frontendCov = 'N/A';
            try {
              const lcov = fs.readFileSync('frontend/coverage/lcov.info', 'utf8');
              const lf = (lcov.match(/^LF:(\d+)/gm) || []).reduce((s, l) => s + parseInt(l.split(':')[1]), 0);
              const lh = (lcov.match(/^LH:(\d+)/gm) || []).reduce((s, l) => s + parseInt(l.split(':')[1]), 0);
              frontendCov = lf > 0 ? (lh / lf * 100).toFixed(1) + '%' : 'N/A';
            } catch {}

            // Job results ‚Äî skipped jobs are OK when path-filtered
            const jobs = {
              backend: '${{ needs.backend.result }}',
              integration: '${{ needs.integration.result }}',
              frontend: '${{ needs.frontend.result }}',
              'frontend-build': '${{ needs.frontend-build.result }}',
              e2e: '${{ needs.e2e.result }}',
              'lint-audit': '${{ needs.lint-audit.result }}'
            };

            const icon = (r) => r === 'success' ? '‚úÖ' : r === 'skipped' ? '‚è≠Ô∏è' : '‚ùå';

            const body = `## ü§ñ CI Summary

            | Job | Status |
            |-----|--------|
            | Lint & Audit | ${icon(jobs['lint-audit'])} ${jobs['lint-audit']} |
            | Backend Tests (4 shards) | ${icon(jobs.backend)} ${jobs.backend} |
            | Integration & Migration | ${icon(jobs.integration)} ${jobs.integration} |
            | Frontend Tests (2 shards) | ${icon(jobs.frontend)} ${jobs.frontend} |
            | Frontend Build | ${icon(jobs['frontend-build'])} ${jobs['frontend-build']} |
            | E2E Tests | ${icon(jobs.e2e)} ${jobs.e2e} |

            ### üìä Coverage
            - **Backend:** ${backendCov}
            - **Frontend:** ${frontendCov}

            ---
            *Updated: ${new Date().toISOString()}*`;

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('ü§ñ CI Summary')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body
              });
            }

      - name: Check all jobs passed
        if: always()
        run: |
          # Skipped jobs are OK (path-filtered), only fail on actual failures
          check_job() {
            local result="$1"
            local name="$2"
            if [ "$result" != "success" ] && [ "$result" != "skipped" ]; then
              echo "‚ùå $name failed: $result"
              return 1
            fi
          }

          FAILED=0
          check_job "${{ needs.backend.result }}" "backend" || FAILED=1
          check_job "${{ needs.integration.result }}" "integration" || FAILED=1
          check_job "${{ needs.frontend.result }}" "frontend" || FAILED=1
          check_job "${{ needs.frontend-build.result }}" "frontend-build" || FAILED=1
          check_job "${{ needs.e2e.result }}" "e2e" || FAILED=1
          check_job "${{ needs.lint-audit.result }}" "lint-audit" || FAILED=1

          if [ "$FAILED" = "1" ]; then
            exit 1
          fi
          echo "‚úÖ All jobs passed (or were skipped due to path filters)"
